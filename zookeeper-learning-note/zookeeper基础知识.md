## zookeeper

Zookeeper 是一个apache发行的，分布式架构的管理、维护框架，其中的数据以节点的形式储存，并且储存量较小，所以适用于一些读多写少的场景，用来保存一些配置相关的信息。

---

#### 下载和安装

---

> 用wget获取压缩包：
>
> ```shell
> wget https://archive.apache.org/dist/zookeeper/zookeeper-3.5.7/apache-zookeeper-3.5.7-bin.tar.gz
> ```
>
> 解压：
>
> ```shell
> tar -zxvf apache-zookeeper-3.5.7-bin.tar.gz
> ```
>
> 解压后在解压目录下会多出一个名为 ``apache-zookeeper-3.5.7-bin`` 的文件夹.
>
> 文件夹中包括：
>
> - bin目录 ：可执行文件放置目录
> - conf ：配置文件放置目录
> - docs : 一些教程
> - lib ：依赖的jar包（zookeeper是用java写的）
> - ....

---

#### 入门配置

---

>在conf目录下会有一个zoo_sample.cfg文件，是配置文件的模版，我们将其更名为zoo.cfg即可生效。
>
>改文件中的一些配置参数：
>
>- tickTime : 通信心跳时间
>- initLimit : 初始化leader和follower建立通讯的最大时间（几次心跳）
>- syncLimit : 同步通讯最大时间，即非初始化时leader和follower保持通讯的最大时间
>- datadir : Zookeeper中数据保存的位置 (一般来说会做修改)
>- clientPort : 客户端链接端口号

---

#### 一些本地命令

---

```shell
#启动服务端
./bin/zkServer.sh start
#关闭服务端
./bin/zkServer.sh stop
#启动客户端连接
bin/zkCli.sh
```

---

#### 集群配置：

---

首先, 需要三台服务器. 

这里我申请了三台云服务器, 并在每台服务器上按照上述流程安装了zookeeper.

然后，在每个服务器的zkData( zoo.cfg文件中的数据保存目录 )文件夹中新建myid文件,在文件中写入自己的id号(自定义)，这里分别用了2，3，4.

然后在每个zookeeper的zoo.cfg文件中添加如下内容.

```shell
server.2=[2001:67c:1254:fd::7e25]:2888:3888
server.3=[2001:67c:1254:e:8c:498:0:1]:2888:3888
server.4=[2001:67c:1254:5f:d58a::f2c1]:2888:3888
```

不难看出，配置的格式为：

```shell
server.{id号}={ip地址}:{端口1}:{端口2}
```

其中，端口1为服务器间通讯用的端口号，端口2为失去链接时，重新选举用的端口号。

---

#### 选举机制

---

##### 第一次选举：

当服务器集群启动时，会进行第一次选举投出第一次的leader，每个服务器启动时会做出如下操作：

1. 先将自己手中的一票投给自己
2. 判断自己手中的选票是否过半，如果过半，自己成为leader，其他服务器默认成为follwer
3. 如果没有过半，则在下一个服务器启动时进行一次比较，将现在已启动服务器中票数最多的那个（也是唯一一个有票的），和现在刚启动的服务器，两者的id进行比较，较小的那个把手中所有的选票交给较大的那个。
4. 结束后，判断手中的选票是否过半，同理，已过半则成为leader，没有的话则继续上述流程。

---

##### 一些概念

---

SID：服务器id，即之前写入的myid

ZXID：每次客户访问的事务id，代表服务器状态变更

Epoch : 每个leader任期的值，每次选举后增加

---

##### 后续选举：

---

当集群中有服务器失去链接时，且失去的是leader时，需要进行重新选举，选举规则如下：

1. 优先比较epoch，大的胜出
2. 然后比较zxid，大的胜出
3. 都一样再比较sid，大的胜出

---

#### 客户端相关操作

---

连接不同(非本地)的集群中其他的服务器：

```
./bin/zkCli.sh server {ip}:2181
```

一些命令：

```shell
##帮助
help

##查看结点信息 默认跟节点名为 zookeeper
ls /

##更详细信息
ls -s /
## czxid 创建节点时的事务id
## ctime znode创建的毫秒数
## zxid  最后一次事务id
## mtime 最后一次修改的毫秒数
## pzxid 最后更新子节点的zxid
## cversion znode子节点的修改次数
## dataversion 数据变化版本
## datalength 数据长度
## numChildren 子节点数量

#新增 create 路径 内容
create /test “content”

#修改内容
set /test "content2"

#删除
delete /test

#删除包括所有子节点
deleteall /test

#单纯查看节点状态
stat /test
```

---

#### zookeeper节点类型

---

节点类型：

- Persistent 持久类型节点，在断开连接后仍保存
- Ephemeral 暂时节点，断开连接后消息 （在create时加上参数-e）

上述两类节点都可以再分为普通类型以及顺序编号节点，顺序编号就是在节点名后面加上一串序号，这串序号是递增的，随着该节点的接入次数增加。在create时加上 -s 参数就可以创建带序号的节点。（注意持久和暂时节点是共用编号顺序的）

---

#### zookeeper 监听器

---

##### 监听原理：

> 在客户端中，会有两个线程，一个负责网络通讯( connect )，另一个负责监听 ( listen )。
>
> Connect 线程将要监听的节点发送给 Zookeeper服务端, 服务端一旦监听到了某个znode的变化，就会将变化发送给客户端，客户端的listen线程将会读取该变化。
>
> 命令如下：
>
> ```
> get -w /test
> ```
>
> 注意：每次只能监听一次变化，想要再次监听变化，需要重新调用该命令

---

#### 一些实战场景

---

1. 服务器的动态上下线 :

   >在常见的网络服务中，我们一般会有多个服务器提供服务。对于不同的客户端，会连接到不同的服务器。当服务器的状态发生变化时候，我们可以用zookeeper通知服客户端。
   >
   >例如：假设目前客户端正在访问server1，突然server1崩溃下线了，这时server1会通知zookeeper，更改zookeeper的znode信息，一旦znode信息发生变化，客户端就会检测到，并且做出相应改动，例如将访问服务器更换到server2.

2. 分布式锁的实现

   >在分布式情况下，数据被保存在多台服务器中，当我们要对数据进行修改时，如果访问的是不同的服务器，会出现数据不同步而造成的错误，这时候可以用zookeeper来对数据进行加锁。
   >
   >例如：当我们想要对数据进行访问的时候，我们需要先去zookeeper中的一个znode里进行一个序列注册，然后判断自己的序列号是否是所有序列中最小的那个，如果是，则继续对数据进行访问，如果不是，则等待自己的上一个序列号的znode被释放，然后进行访问。最后，再将自己的znode删除。

----

#### 生产环境中比较适合的zookeeper服务器数量

---

10台服务器：3台zookeeper服务器

20台服务器：5台zookeeper服务器>=

如果>=100台服务器：11台zookeeper服务器











